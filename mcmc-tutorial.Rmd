---
title: "MCMC Tutorial"
subtitle: "A short introduction to Bayesian Analysis and Metropolis-Hastings MCMC"
author: "Ralph Schlosser"
date: "February 2017"
output:
  beamer_presentation:
    incremental: false
    includes:
      in_header: [mystyle.tex, settings.tex]
    keep_tex: true
---

## Overview

- Bayesian Analysis
- Monte Carlo Integration
- Sampling Methods
- Markov Chain Monte Carlo
- Metropolis-Hastings Algorithm
- Example: Linear Regression and M-H MCMC
- Outlook

## Bayesian Analysis: Introduction

\begin{columns}
  \column{0.6\textwidth}
  \begin{itemize}
    \item Foundation for MCMC: \HIGHLIGHT{Bayesian Analysis}.
    \item \HIGHLIGHT{Frequentist} -- Likelihood model: $p(\vec{x} | \vec{\theta})$
    \item How likely are the data $\vec{x}$, given the fixed parameters $\vec{\theta}$.
    \item In general we want to estimate $\vec{\theta}$, e.g. through MLE.
    \item \HIGHLIGHT{Bayesian} -- Bayesian model: $p(\vec{\theta} | \vec{x})$.
    \item Fundamental difference: In Bayesian analysis, both parameter model \orange{and} data are treated as random variables.
  \end{itemize}
  \column{0.4\textwidth}
  \begin{figure}
    \includegraphics[scale=0.5]{images/bayes.png}
    \caption*{\tiny Thomas Bayes (1707-1761)}
  \end{figure}
\end{columns}

## Bayesian Analysis: Terminology

- From \HIGHLIGHT{joint probability} distribution to \HIGHLIGHT{posterior distribution} via data \HIGHLIGHT{likelihood} and \HIGHLIGHT{prior beliefs}:

$$
\begin{aligned}
p(x, \theta) &= p(x | \theta) p(\theta)\\
p(\theta | x) &= \frac{p(x, \theta)}{p(x)}\\
&= \frac{p(x | \theta) p(\theta)}{p(x)} 
\end{aligned}
$$

- Normalizing term $p(x)$ difficult to get, but often not needed:

$$
\begin{aligned}
p(\theta | x) & \propto p(x | \theta) p(\theta)
\end{aligned}
$$

- \orange{Posterior $\propto$ likelihood $\times$ prior}


## Bayesian Analysis: Pros and Cons

- \HIGHLIGHT{Pro}: Common-sense interpretability of results, e.g. *Credible Intervals* vs. classical Confidence Intervals.
- \HIGHLIGHT{Pro}: Update model parameters as new data becomes available.
- \HIGHLIGHT{Pro}: Create *hierarchical models* through chaining: 

    - $p(\phi, \theta | x) = p(x |\phi, \theta) p(\theta | \phi) p(\phi)$
    - Hyperprior: $p(\theta | \phi) p(\phi)$
    - *Yesterdays posterior is tomorrow's prior*
    
- \orange{Con}: **Must** have a joint model for parameters, data, and prior. 
    - What if we have absolutely no prior information?
- \orange{Con}: Choice of prior considered to be subjective.
- \orange{Con}: Subjectiveness makes comparison difficult.


## Bayesian Analysis: Applications

- Inferences and predictions in a Bayesian setting:

$$
\begin{aligned}
p(\theta | x) &= \frac{p(x | \theta) p(\theta)}{\int_{\Theta} p(x | \theta') p(\theta')  d \theta'} & \mbox{Normalization}\\
p(\tilde{y} | y) &= \int_{\Theta} p(\tilde{y} | \theta') p(\theta' | y)  d \theta' & \mbox{Predict new data}
\end{aligned}
$$

- Posterior summary statistics, e.g. expectations:

$$
\begin{aligned}
\mathbb{E}_{p}(g(\theta) | x) &= \int_{\Theta} g(\theta') p(\theta' | x) d\theta'\\
\mbox{mean: } g(\theta) = \theta
\end{aligned}
$$

- Many classical models can be expressed in a Bayesian context, like e.g. linear regression, ARMA, GLMs, etc. 
- \HIGHLIGHT{Missing data}: Natural extension.


## Monte Carlo Integration: Introduction

- Applied Bayesian analysis asks to \orange{integrate} over (often analytically intractable) posterior densities.
- \HIGHLIGHT{Solution}: **Monte Carlo Integration**
- Suppose we wish to evaluate $\mathbb{E}_{p}(g(\theta) | x) = \int_{\Theta} g(\theta') p(\theta' | x) d\theta'$
- Given a set of $N$ \orange{i.i.d. samples} $\theta_1, \theta_2, ..., \theta_N$ from the density $p$:

$$
\begin{aligned}
\mathbb{E}_{p}(g(\theta | x)) & \approx \frac{1}{N} \sum_{i = 1}^{N} g(\theta_i)
\end{aligned}
$$

- \HIGHLIGHT{But}: Need to be able to draw random samples from $p$!

## Monte Carlo Integration: Example

Simulate $N=10000$ draws from a univariate standard normal, i.e. $X \sim N(0, 1)$. Let $p(x)$ be the normal density. Then:

$$
\begin{aligned}
P(X \le 0.5) &= \int_{-\infty}^{0.5} p(x) dx
\end{aligned}
$$


```{r}
set.seed(123)
data <- rnorm(n = 10000)
prob.in <- data <= 0.5
sum(prob.in) / 10000
pnorm(0.5)
```


## Sampling Methods

- Sampling from the posterior distribution is really important.
- Classical sampling methods:
    - Inversion sampling
    - Importance sampling
    - Rejection sampling
- Drawbacks:
    - Closed-form expression rarely accessible (Method of Inversion).
    - Doesn't generalize well for \orange{highly-dimensional} problems.
- \HIGHLIGHT{Metropolis-Hastings MCMC} has largely superseded the above.

## Markov Chain Monte Carlo (MCMC)

- Unlike pure Monte Carlo, in MCMC we create \HIGHLIGHT{dependent} samples. 
- Consider the **target distribution** $p(\theta | x)$ which is only known up to proportionality. 
- Construct a Markov Chain in the state space of $\theta \in \Theta$ with \orange{stationary distribution} $p(\theta | x)$.
- Markov property -- New state of chain depends only on previous state ($K$: transitional kernel d.f.).

$$
\begin{aligned}
\theta_{t + 1} &= K(\theta | \theta_t) \\
\end{aligned}
$$

- With realizations $\{\theta_t: t=0, 1, ...\}$ from the chain:

$$
\begin{aligned}
\theta_t & \rightarrow p(\theta | x) \\
\frac{1}{N} \sum_{t = 1}^{N} g(\theta_t) & \rightarrow \mathbb{E}_{p}(g(\theta | x)) \mbox{ a.s.}
\end{aligned}
$$

## Metropolis-Hastings MCMC: Intro & some history

- An implementation of MCMC. 
- Originally developed by researchers \HIGHLIGHT{Nicholas Metropolis, Stanislaw Ulam}, and co. at Los Alamos National Laboratories in the 1950's.
- Generalized through work done by \orange{Hastings} in the 1970's.
- Popularized by a 1990 research paper from \HIGHLIGHT{Gelfand \& Smith}: \url{http://wwwf.imperial.ac.uk/~das01/MyWeb/SCBI/Papers/GelfandSmith.pdf}
- M-H MCMC really helped turning Bayesian analysis into practically useful tool.

## Metropolis-Hastings MCMC: Terminology

- M-H has two main ingredients.
- A \HIGHLIGHT{proposal distribution}.
    - Dependent on the current chain state $\theta_t$, generate a candidate for the new state $\phi$.
    - Written as $q(\theta_t, \phi)$.
    - Can be chosen arbitrarily, but there are caveats (efficiency).
- An \orange{acceptance probability}. 
    - Accept with probability $\alpha$ the move from the current state $\theta_t$ to state $\phi$.
    - Written as $\alpha(\theta_t, \phi)$.
    
- Main idea behind M-H: With every step, we want to get closer to the target density (e.g. posterior density).

## Metropolis-Hastings MCMC: Intuition

- Let's call our \orange{target distribution} (from which we want to sample) $\pi$.
- At the core of the M-H algorithm we have the calculation of $\alpha(\theta_t, \phi)$:

$$
\begin{aligned}
\alpha(\theta_t, \phi) = min \Big( 1, \frac{\pi(\phi) q(\phi, \theta_t)}{\pi(\theta_t) q(\theta_t, \phi)} \Big)
\end{aligned}
$$

- Often $q$ is symmetric, in which case it cancels out.
- If $\frac{\pi(\phi)}{\pi({\theta_t})} > 1$ $\rightarrow$ target density at the proposed **new** value is higher than at current value.
- In this case, we will \HIGHLIGHT{accept} the move from $\theta_t$ to $\phi$ with probability 1.
    - *M-H really loves upward moves* :)
- **Main point**: Working with ratios of $\pi$, so only need $\pi$ up to proportionality!

## Metropolis-Hastings MCMC: Algorithm

1. Initialize $\theta_0$, number of iterations.
1. Given the current state $\theta_t$, generate new state $\phi$ from the proposal distribution $q(\theta_t, \phi)$.
2. Calculate acceptance probability $\alpha(\theta_t, \phi)$.
3. With probability $\alpha(\theta_t, \phi)$, set $\theta_{t + 1} = \phi$, else set $\theta_{t + 1} = \theta_t$.
4. Iterate
5. Result: \HIGHLIGHT{Realizations} of dependent samples $\{\theta_1, \theta_2, ...\}$ from the target distribution $\pi(\theta)$.

Using these dependent realizations & due to the Monte Carlo approach, we can now look at making inferences and predictions.


## Example: Linear Regression and M-H MCMC

- Consider a simple linear model: $y = \beta_1 x + \epsilon$.
- As usual $\epsilon \sim N(0, \sigma^2)$ with $\sigma^2$ known.
- We wish to make inferences on, e.g. $\beta_1$.
- Bayesian approach:

$$
\begin{aligned}
p(\beta_1 | y, x, \sigma^2) & = p(y | \beta_1, x, \sigma^2) p(\beta_1)
\end{aligned}
$$

- Let's choose a uniform prior for $\beta_1$. We can now create samples using M-H MCMC.
- See R code!

## Outlook

- Many more interesting things could be mentioned, e.g. \orange{burn-in}, choice of $q$, Gibbs-sampling etc.
- M-H and Monte Carlo in deep learning: \url{http://www.deeplearningbook.org/contents/monte_carlo.html}
- Bayesian Deep Learning is a \HIGHLIGHT{thing} (apparently, don't know anything about it!)
- Went way over my head, but looks cool -- Finding the Higgs boson, featuring Monte Carlo & Bayes: \url{http://hea-www.harvard.edu/AstroStat/Stat310_1314/dvd_20140121.pdf}
- Along the same lines, the amazing NIPS 2016 keynote: \url{https://nips.cc/Conferences/2016/Schedule?showEvent=6195}
- M-H in \orange{Latent Dirichlet Allocation}: \url{http://mlg.eng.cam.ac.uk/teaching/4f13/1112/lect10.pdf}
